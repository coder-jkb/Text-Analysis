{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "import pyphen # for syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction\n",
    "> ### web scraping (using bs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-telehealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telemedici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-people-di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/is-telehealth...\n",
       "1       2  https://insights.blackcoffer.com/how-telehealt...\n",
       "2       3  https://insights.blackcoffer.com/is-telemedici...\n",
       "3       4  https://insights.blackcoffer.com/is-telehealth...\n",
       "4       5  https://insights.blackcoffer.com/how-people-di..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Input.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   URL_ID  150 non-null    int64 \n",
      " 1   URL     150 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags to be extracted\n",
    "- title : `h1.entry-title`\n",
    "- content : `div.td-post-content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "for index in df.index:\n",
    "    \n",
    "    # extracting HTML\n",
    "    url = df['URL'][index]\n",
    "    htmlContent = requests.get(url, headers=headers).content\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "\n",
    "    # extracting content\n",
    "    title = soup.find(\"h1\", class_=\"entry-title\").text\n",
    "    content = soup.find(\"div\", class_=\"td-post-content\").text\n",
    "    \n",
    "    # writing in file\n",
    "    file_name = str(df['URL_ID'][index])+'.txt'\n",
    "    with open(file_name, 'w') as f:\n",
    "        # encode into utf-8 to remove an error while scraping the text\n",
    "        title = title.encode(encoding = 'utf-8')\n",
    "        content = content.encode(encoding = 'utf-8')\n",
    "        f.write(f'{title}\\n{content}')\n",
    "\n",
    "    # to remove this b' from byte string \n",
    "    with open(file_name,'r') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    l1 = text[0][:-2].replace(\"b'\",\"\")\n",
    "    l2 = text[1][:-1].replace(\"b'\",\"\")\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(f\"{l1}\\n{l2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Made list of positive and negative words from given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negative-words.txt','r') as f:\n",
    "    words = f.read()\n",
    "    neg = words.split(\"\\n\")\n",
    "\n",
    "with open('positive-words.txt','r') as f:\n",
    "    words = f.read()\n",
    "    pos = words.split(\"\\n\")\n",
    "\n",
    "dictionary = {  \"positive\": pos,\n",
    "                \"negative\": neg } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading StopWords files, and making `list` of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smith',\n",
       " 'surnames',\n",
       " 'from',\n",
       " 'census',\n",
       " 'johnson',\n",
       " 'williams',\n",
       " 'jones',\n",
       " 'brown',\n",
       " 'davis',\n",
       " 'miller',\n",
       " 'wilson',\n",
       " 'moore',\n",
       " 'taylor',\n",
       " 'anderson',\n",
       " 'thomas',\n",
       " 'jackson',\n",
       " 'white',\n",
       " 'harris',\n",
       " 'martin',\n",
       " 'thompson',\n",
       " 'garcia',\n",
       " 'martinez',\n",
       " 'robinson',\n",
       " 'clark',\n",
       " 'rodriguez',\n",
       " 'lewis',\n",
       " 'lee',\n",
       " 'walker',\n",
       " 'hall',\n",
       " 'allen',\n",
       " 'young',\n",
       " 'hernandez',\n",
       " 'king',\n",
       " 'wright',\n",
       " 'lopez',\n",
       " 'hill',\n",
       " 'scott',\n",
       " 'green',\n",
       " 'adams',\n",
       " 'baker',\n",
       " 'gonzalez',\n",
       " 'nelson',\n",
       " 'carter',\n",
       " 'mitchell',\n",
       " 'perez',\n",
       " 'roberts',\n",
       " 'turner',\n",
       " 'phillips',\n",
       " 'campbell',\n",
       " 'parker',\n",
       " 'evans',\n",
       " 'edwards',\n",
       " 'collins',\n",
       " 'stewart',\n",
       " 'sanchez',\n",
       " 'morris',\n",
       " 'rogers',\n",
       " 'reed',\n",
       " 'cook',\n",
       " 'morgan',\n",
       " 'bell',\n",
       " 'murphy',\n",
       " 'bailey',\n",
       " 'rivera',\n",
       " 'cooper',\n",
       " 'richardson',\n",
       " 'cox',\n",
       " 'howard',\n",
       " 'ward',\n",
       " 'torres',\n",
       " 'peterson',\n",
       " 'gray',\n",
       " 'ramirez',\n",
       " 'james',\n",
       " 'watson',\n",
       " 'brooks',\n",
       " 'kelly',\n",
       " 'sanders',\n",
       " 'price',\n",
       " 'bennett',\n",
       " 'wood',\n",
       " 'barnes',\n",
       " 'ross',\n",
       " 'henderson',\n",
       " 'coleman',\n",
       " 'jenkins',\n",
       " 'perry',\n",
       " 'powell',\n",
       " 'long',\n",
       " 'patterson',\n",
       " 'hughes',\n",
       " 'flores',\n",
       " 'washington',\n",
       " 'butler',\n",
       " 'simmons',\n",
       " 'foster',\n",
       " 'gonzales',\n",
       " 'bryant',\n",
       " 'alexander',\n",
       " 'russell',\n",
       " 'griffin',\n",
       " 'diaz',\n",
       " 'hayes',\n",
       " 'myers',\n",
       " 'ford',\n",
       " 'hamilton',\n",
       " 'graham',\n",
       " 'sullivan',\n",
       " 'wallace',\n",
       " 'woods',\n",
       " 'cole',\n",
       " 'west',\n",
       " 'jordan',\n",
       " 'owens',\n",
       " 'reynolds',\n",
       " 'fisher',\n",
       " 'ellis',\n",
       " 'harrison',\n",
       " 'gibson',\n",
       " 'mcdonald',\n",
       " 'cruz',\n",
       " 'marshall',\n",
       " 'ortiz',\n",
       " 'gomez',\n",
       " 'murray',\n",
       " 'freeman',\n",
       " 'wells',\n",
       " 'webb',\n",
       " 'simpson',\n",
       " 'stevens',\n",
       " 'tucker',\n",
       " 'porter',\n",
       " 'hunter',\n",
       " 'hicks',\n",
       " 'crawford',\n",
       " 'henry',\n",
       " 'boyd',\n",
       " 'mason',\n",
       " 'morales',\n",
       " 'kennedy',\n",
       " 'warren',\n",
       " 'dixon',\n",
       " 'ramos',\n",
       " 'reyes',\n",
       " 'burns',\n",
       " 'gordon',\n",
       " 'shaw',\n",
       " 'holmes',\n",
       " 'rice',\n",
       " 'robertson',\n",
       " 'hunt',\n",
       " 'black',\n",
       " 'daniels',\n",
       " 'palmer',\n",
       " 'mills',\n",
       " 'nichols',\n",
       " 'grant',\n",
       " 'knight',\n",
       " 'ferguson',\n",
       " 'rose',\n",
       " 'stone',\n",
       " 'hawkins',\n",
       " 'dunn',\n",
       " 'perkins',\n",
       " 'hudson',\n",
       " 'spencer',\n",
       " 'gardner',\n",
       " 'stephens',\n",
       " 'payne',\n",
       " 'pierce',\n",
       " 'berry',\n",
       " 'matthews',\n",
       " 'arnold',\n",
       " 'wagner',\n",
       " 'willis',\n",
       " 'ray',\n",
       " 'watkins',\n",
       " 'olson',\n",
       " 'carroll',\n",
       " 'duncan',\n",
       " 'snyder',\n",
       " 'hart',\n",
       " 'cunningham',\n",
       " 'bradley',\n",
       " 'lane',\n",
       " 'andrews',\n",
       " 'ruiz',\n",
       " 'harper',\n",
       " 'fox',\n",
       " 'riley',\n",
       " 'armstrong',\n",
       " 'carpenter',\n",
       " 'weaver',\n",
       " 'greene',\n",
       " 'lawrence',\n",
       " 'elliott',\n",
       " 'chavez',\n",
       " 'sims',\n",
       " 'austin',\n",
       " 'peters',\n",
       " 'kelley',\n",
       " 'franklin',\n",
       " 'lawson',\n",
       " 'fields',\n",
       " 'gutierrez',\n",
       " 'ryan',\n",
       " 'schmidt',\n",
       " 'carr',\n",
       " 'vasquez',\n",
       " 'castillo',\n",
       " 'wheeler',\n",
       " 'chapman',\n",
       " 'oliver',\n",
       " 'montgomery',\n",
       " 'richards',\n",
       " 'williamson',\n",
       " 'johnston',\n",
       " 'banks',\n",
       " 'meyer',\n",
       " 'bishop',\n",
       " 'mccoy',\n",
       " 'howell',\n",
       " 'alvarez',\n",
       " 'morrison',\n",
       " 'hansen',\n",
       " 'fernandez',\n",
       " 'garza',\n",
       " 'harvey',\n",
       " 'little',\n",
       " 'burton',\n",
       " 'stanley',\n",
       " 'nguyen',\n",
       " 'george',\n",
       " 'jacobs',\n",
       " 'reid',\n",
       " 'kim',\n",
       " 'fuller',\n",
       " 'lynch',\n",
       " 'dean',\n",
       " 'gilbert',\n",
       " 'garrett',\n",
       " 'romero',\n",
       " 'welch',\n",
       " 'larson',\n",
       " 'frazier',\n",
       " 'burke',\n",
       " 'hanson',\n",
       " 'day',\n",
       " 'mendoza',\n",
       " 'moreno',\n",
       " 'bowman',\n",
       " 'medina',\n",
       " 'fowler',\n",
       " 'brewer',\n",
       " 'hoffman',\n",
       " 'carlson',\n",
       " 'silva',\n",
       " 'pearson',\n",
       " 'holland',\n",
       " 'douglas',\n",
       " 'fleming',\n",
       " 'jensen',\n",
       " 'vargas',\n",
       " 'byrd',\n",
       " 'davidson',\n",
       " 'hopkins',\n",
       " 'may',\n",
       " 'terry',\n",
       " 'herrera',\n",
       " 'wade',\n",
       " 'soto',\n",
       " 'walters',\n",
       " 'curtis',\n",
       " 'neal',\n",
       " 'caldwell',\n",
       " 'lowe',\n",
       " 'jennings',\n",
       " 'barnett',\n",
       " 'graves',\n",
       " 'jimenez',\n",
       " 'horton',\n",
       " 'shelton',\n",
       " 'barrett',\n",
       " 'obrien',\n",
       " 'castro',\n",
       " 'sutton',\n",
       " 'gregory',\n",
       " 'mckinney',\n",
       " 'lucas',\n",
       " 'miles',\n",
       " 'craig',\n",
       " 'rodriquez',\n",
       " 'chambers',\n",
       " 'holt',\n",
       " 'lambert',\n",
       " 'fletcher',\n",
       " 'watts',\n",
       " 'bates',\n",
       " 'hale',\n",
       " 'rhodes',\n",
       " 'pena',\n",
       " 'beck',\n",
       " 'newman',\n",
       " 'haynes',\n",
       " 'mcdaniel',\n",
       " 'mendez',\n",
       " 'bush',\n",
       " 'vaughn',\n",
       " 'parks',\n",
       " 'dawson',\n",
       " 'santiago',\n",
       " 'norris',\n",
       " 'hardy',\n",
       " 'love',\n",
       " 'steele',\n",
       " 'curry',\n",
       " 'powers',\n",
       " 'schultz',\n",
       " 'barker',\n",
       " 'guzman',\n",
       " 'page',\n",
       " 'munoz',\n",
       " 'ball',\n",
       " 'keller',\n",
       " 'chandler',\n",
       " 'weber',\n",
       " 'leonard',\n",
       " 'walsh',\n",
       " 'lyons',\n",
       " 'ramsey',\n",
       " 'wolfe',\n",
       " 'schneider',\n",
       " 'mullins',\n",
       " 'benson',\n",
       " 'sharp',\n",
       " 'bowen',\n",
       " 'daniel',\n",
       " 'barber',\n",
       " 'cummings',\n",
       " 'hines',\n",
       " 'baldwin',\n",
       " 'griffith',\n",
       " 'valdez',\n",
       " 'hubbard',\n",
       " 'salazar',\n",
       " 'reeves',\n",
       " 'warner',\n",
       " 'stevenson',\n",
       " 'burgess',\n",
       " 'santos',\n",
       " 'tate',\n",
       " 'cross',\n",
       " 'garner',\n",
       " 'mann',\n",
       " 'mack',\n",
       " 'moss',\n",
       " 'thornton',\n",
       " 'dennis',\n",
       " 'mcgee',\n",
       " 'farmer',\n",
       " 'delgado',\n",
       " 'aguilar',\n",
       " 'vega',\n",
       " 'glover',\n",
       " 'manning',\n",
       " 'cohen',\n",
       " 'harmon',\n",
       " 'rodgers',\n",
       " 'robbins',\n",
       " 'newton',\n",
       " 'todd',\n",
       " 'blair',\n",
       " 'higgins',\n",
       " 'ingram',\n",
       " 'reese',\n",
       " 'cannon',\n",
       " 'strickland',\n",
       " 'townsend',\n",
       " 'potter',\n",
       " 'goodwin',\n",
       " 'walton',\n",
       " 'rowe',\n",
       " 'hampton',\n",
       " 'ortega',\n",
       " 'patton',\n",
       " 'swanson',\n",
       " 'joseph',\n",
       " 'francis',\n",
       " 'goodman',\n",
       " 'maldonado',\n",
       " 'yates',\n",
       " 'becker',\n",
       " 'erickson',\n",
       " 'hodges',\n",
       " 'rios',\n",
       " 'conner',\n",
       " 'adkins',\n",
       " 'webster',\n",
       " 'norman',\n",
       " 'malone',\n",
       " 'hammond',\n",
       " 'flowers',\n",
       " 'cobb',\n",
       " 'moody',\n",
       " 'quinn',\n",
       " 'blake',\n",
       " 'maxwell',\n",
       " 'pope',\n",
       " 'floyd',\n",
       " 'osborne',\n",
       " 'paul',\n",
       " 'mccarthy',\n",
       " 'guerrero',\n",
       " 'lindsey',\n",
       " 'estrada',\n",
       " 'sandoval',\n",
       " 'gibbs',\n",
       " 'tyler',\n",
       " 'gross',\n",
       " 'fitzgerald',\n",
       " 'stokes',\n",
       " 'doyle',\n",
       " 'sherman',\n",
       " 'saunders',\n",
       " 'wise',\n",
       " 'colon',\n",
       " 'gill',\n",
       " 'alvarado',\n",
       " 'greer',\n",
       " 'padilla',\n",
       " 'simon',\n",
       " 'waters',\n",
       " 'nunez',\n",
       " 'ballard',\n",
       " 'schwartz',\n",
       " 'mcbride',\n",
       " 'houston',\n",
       " 'christensen',\n",
       " 'klein',\n",
       " 'pratt',\n",
       " 'briggs',\n",
       " 'parsons',\n",
       " 'mclaughlin',\n",
       " 'zimmerman',\n",
       " 'french',\n",
       " 'buchanan',\n",
       " 'moran',\n",
       " 'copeland',\n",
       " 'roy',\n",
       " 'pittman',\n",
       " 'brady',\n",
       " 'mccormick',\n",
       " 'holloway',\n",
       " 'brock',\n",
       " 'poole',\n",
       " 'frank',\n",
       " 'logan',\n",
       " 'owen',\n",
       " 'bass',\n",
       " 'marsh',\n",
       " 'drake',\n",
       " 'wong',\n",
       " 'jefferson',\n",
       " 'park',\n",
       " 'morton',\n",
       " 'abbott',\n",
       " 'sparks',\n",
       " 'patrick',\n",
       " 'norton',\n",
       " 'huff',\n",
       " 'clayton',\n",
       " 'massey',\n",
       " 'lloyd',\n",
       " 'figueroa',\n",
       " 'carson',\n",
       " 'bowers',\n",
       " 'roberson',\n",
       " 'barton',\n",
       " 'tran',\n",
       " 'lamb',\n",
       " 'harrington',\n",
       " 'casey',\n",
       " 'boone',\n",
       " 'cortez',\n",
       " 'clarke',\n",
       " 'mathis',\n",
       " 'singleton',\n",
       " 'wilkins',\n",
       " 'cain',\n",
       " 'bryan',\n",
       " 'underwood',\n",
       " 'hogan',\n",
       " 'mckenzie',\n",
       " 'collier',\n",
       " 'luna',\n",
       " 'phelps',\n",
       " 'mcguire',\n",
       " 'allison',\n",
       " 'bridges',\n",
       " 'wilkerson',\n",
       " 'nash',\n",
       " 'summers',\n",
       " 'atkins',\n",
       " 'wilcox',\n",
       " 'pitts',\n",
       " 'conley',\n",
       " 'marquez',\n",
       " 'burnett',\n",
       " 'richard',\n",
       " 'cochran',\n",
       " 'chase',\n",
       " 'davenport',\n",
       " 'hood',\n",
       " 'gates',\n",
       " 'clay',\n",
       " 'ayala',\n",
       " 'sawyer',\n",
       " 'roman',\n",
       " 'vazquez',\n",
       " 'dickerson',\n",
       " 'hodge',\n",
       " 'acosta',\n",
       " 'flynn',\n",
       " 'espinoza',\n",
       " 'nicholson',\n",
       " 'monroe',\n",
       " 'wolf',\n",
       " 'morrow',\n",
       " 'kirk',\n",
       " 'randall',\n",
       " 'anthony',\n",
       " 'whitaker',\n",
       " 'oconnor',\n",
       " 'skinner',\n",
       " 'ware',\n",
       " 'molina',\n",
       " 'kirby',\n",
       " 'huffman',\n",
       " 'bradford',\n",
       " 'charles',\n",
       " 'gilmore',\n",
       " 'dominguez',\n",
       " 'oneal',\n",
       " 'bruce',\n",
       " 'lang',\n",
       " 'combs',\n",
       " 'kramer',\n",
       " 'heath',\n",
       " 'hancock',\n",
       " 'gallagher',\n",
       " 'gaines',\n",
       " 'shaffer',\n",
       " 'short',\n",
       " 'wiggins',\n",
       " 'mathews',\n",
       " 'mcclain',\n",
       " 'fischer',\n",
       " 'wall',\n",
       " 'small',\n",
       " 'melton',\n",
       " 'hensley',\n",
       " 'bond',\n",
       " 'dyer',\n",
       " 'cameron',\n",
       " 'grimes',\n",
       " 'contreras',\n",
       " 'christian',\n",
       " 'wyatt',\n",
       " 'baxter',\n",
       " 'snow',\n",
       " 'mosley',\n",
       " 'shepherd',\n",
       " 'larsen',\n",
       " 'hoover',\n",
       " 'beasley',\n",
       " 'glenn',\n",
       " 'petersen',\n",
       " 'whitehead',\n",
       " 'meyers',\n",
       " 'keith',\n",
       " 'garrison',\n",
       " 'vincent',\n",
       " 'shields',\n",
       " 'horn',\n",
       " 'savage',\n",
       " 'olsen',\n",
       " 'schroeder',\n",
       " 'hartman',\n",
       " 'woodard',\n",
       " 'mueller',\n",
       " 'kemp',\n",
       " 'deleon',\n",
       " 'booth',\n",
       " 'patel',\n",
       " 'calhoun',\n",
       " 'wiley',\n",
       " 'eaton',\n",
       " 'cline',\n",
       " 'navarro',\n",
       " 'harrell',\n",
       " 'lester',\n",
       " 'humphrey',\n",
       " 'parrish',\n",
       " 'duran',\n",
       " 'hutchinson',\n",
       " 'hess',\n",
       " 'dorsey',\n",
       " 'bullock',\n",
       " 'robles',\n",
       " 'beard',\n",
       " 'dalton',\n",
       " 'avila',\n",
       " 'vance',\n",
       " 'rich',\n",
       " 'blackwell',\n",
       " 'york',\n",
       " 'johns',\n",
       " 'blankenship',\n",
       " 'trevino',\n",
       " 'salinas',\n",
       " 'campos',\n",
       " 'pruitt',\n",
       " 'moses',\n",
       " 'callahan',\n",
       " 'golden',\n",
       " 'montoya',\n",
       " 'hardin',\n",
       " 'guerra',\n",
       " 'mcdowell',\n",
       " 'carey',\n",
       " 'stafford',\n",
       " 'gallegos',\n",
       " 'henson',\n",
       " 'wilkinson',\n",
       " 'booker',\n",
       " 'merritt',\n",
       " 'miranda',\n",
       " 'atkinson',\n",
       " 'orr',\n",
       " 'decker',\n",
       " 'hobbs',\n",
       " 'preston',\n",
       " 'tanner',\n",
       " 'knox',\n",
       " 'pacheco',\n",
       " 'stephenson',\n",
       " 'glass',\n",
       " 'rojas',\n",
       " 'serrano',\n",
       " 'marks',\n",
       " 'hickman',\n",
       " 'english',\n",
       " 'sweeney',\n",
       " 'strong',\n",
       " 'prince',\n",
       " 'mcclure',\n",
       " 'conway',\n",
       " 'walter',\n",
       " 'roth',\n",
       " 'maynard',\n",
       " 'farrell',\n",
       " 'lowery',\n",
       " 'hurst',\n",
       " 'nixon',\n",
       " 'weiss',\n",
       " 'trujillo',\n",
       " 'ellison',\n",
       " 'sloan',\n",
       " 'juarez',\n",
       " 'winters',\n",
       " 'mclean',\n",
       " 'randolph',\n",
       " 'leon',\n",
       " 'boyer',\n",
       " 'villarreal',\n",
       " 'mccall',\n",
       " 'gentry',\n",
       " 'carrillo',\n",
       " 'kent',\n",
       " 'ayers',\n",
       " 'lara',\n",
       " 'shannon',\n",
       " 'sexton',\n",
       " 'pace',\n",
       " 'hull',\n",
       " 'leblanc',\n",
       " 'browning',\n",
       " 'velasquez',\n",
       " 'leach',\n",
       " 'chang',\n",
       " 'house',\n",
       " 'sellers',\n",
       " 'herring',\n",
       " 'noble',\n",
       " 'foley',\n",
       " 'bartlett',\n",
       " 'mercado',\n",
       " 'landry',\n",
       " 'durham',\n",
       " 'walls',\n",
       " 'barr',\n",
       " 'mckee',\n",
       " 'bauer',\n",
       " 'rivers',\n",
       " 'everett',\n",
       " 'bradshaw',\n",
       " 'pugh',\n",
       " 'velez',\n",
       " 'rush',\n",
       " 'estes',\n",
       " 'dodson',\n",
       " 'morse',\n",
       " 'sheppard',\n",
       " 'weeks',\n",
       " 'camacho',\n",
       " 'bean',\n",
       " 'barron',\n",
       " 'livingston',\n",
       " 'middleton',\n",
       " 'spears',\n",
       " 'branch',\n",
       " 'blevins',\n",
       " 'chen',\n",
       " 'kerr',\n",
       " 'mcconnell',\n",
       " 'hatfield',\n",
       " 'harding',\n",
       " 'ashley',\n",
       " 'solis',\n",
       " 'herman',\n",
       " 'frost',\n",
       " 'giles',\n",
       " 'blackburn',\n",
       " 'william',\n",
       " 'pennington',\n",
       " 'woodward',\n",
       " 'finley',\n",
       " 'mcintosh',\n",
       " 'koch',\n",
       " 'best',\n",
       " 'solomon',\n",
       " 'mccullough',\n",
       " 'dudley',\n",
       " 'nolan',\n",
       " 'blanchard',\n",
       " 'rivas',\n",
       " 'brennan',\n",
       " 'mejia',\n",
       " 'kane',\n",
       " 'benton',\n",
       " 'joyce',\n",
       " 'buckley',\n",
       " 'haley',\n",
       " 'valentine',\n",
       " 'maddox',\n",
       " 'russo',\n",
       " 'mcknight',\n",
       " 'buck',\n",
       " 'moon',\n",
       " 'mcmillan',\n",
       " 'crosby',\n",
       " 'berg',\n",
       " 'dotson',\n",
       " 'mays',\n",
       " 'roach',\n",
       " 'church',\n",
       " 'chan',\n",
       " 'richmond',\n",
       " 'meadows',\n",
       " 'faulkner',\n",
       " 'oneill',\n",
       " 'knapp',\n",
       " 'kline',\n",
       " 'barry',\n",
       " 'ochoa',\n",
       " 'jacobson',\n",
       " 'gay',\n",
       " 'avery',\n",
       " 'hendricks',\n",
       " 'horne',\n",
       " 'shepard',\n",
       " 'hebert',\n",
       " 'cherry',\n",
       " 'cardenas',\n",
       " 'mcintyre',\n",
       " 'whitney',\n",
       " 'waller',\n",
       " 'holman',\n",
       " 'donaldson',\n",
       " 'cantu',\n",
       " 'terrell',\n",
       " 'morin',\n",
       " 'gillespie',\n",
       " 'fuentes',\n",
       " 'tillman',\n",
       " 'sanford',\n",
       " 'bentley',\n",
       " 'peck',\n",
       " 'key',\n",
       " 'salas',\n",
       " 'rollins',\n",
       " 'gamble',\n",
       " 'dickson',\n",
       " 'battle',\n",
       " 'santana',\n",
       " 'cabrera',\n",
       " 'cervantes',\n",
       " 'howe',\n",
       " 'hinton',\n",
       " 'hurley',\n",
       " 'spence',\n",
       " 'zamora',\n",
       " 'yang',\n",
       " 'mcneil',\n",
       " 'suarez',\n",
       " 'case',\n",
       " 'petty',\n",
       " 'gould',\n",
       " 'mcfarland',\n",
       " 'sampson',\n",
       " 'carver',\n",
       " 'bray',\n",
       " 'rosario',\n",
       " 'macdonald',\n",
       " 'stout',\n",
       " 'hester',\n",
       " 'melendez',\n",
       " 'dillon',\n",
       " 'farley',\n",
       " 'hopper',\n",
       " 'galloway',\n",
       " 'potts',\n",
       " 'bernard',\n",
       " 'joyner',\n",
       " 'stein',\n",
       " 'aguirre',\n",
       " 'osborn',\n",
       " 'mercer',\n",
       " 'bender',\n",
       " 'franco',\n",
       " 'rowland',\n",
       " 'sykes',\n",
       " 'benjamin',\n",
       " 'travis',\n",
       " 'pickett',\n",
       " 'crane',\n",
       " 'sears',\n",
       " 'mayo',\n",
       " 'dunlap',\n",
       " 'hayden',\n",
       " 'wilder',\n",
       " 'mckay',\n",
       " 'coffey',\n",
       " 'mccarty',\n",
       " 'ewing',\n",
       " 'cooley',\n",
       " 'vaughan',\n",
       " 'bonner',\n",
       " 'cotton',\n",
       " 'holder',\n",
       " 'stark',\n",
       " 'ferrell',\n",
       " 'cantrell',\n",
       " 'fulton',\n",
       " 'lynn',\n",
       " 'lott',\n",
       " 'calderon',\n",
       " 'rosa',\n",
       " 'pollard',\n",
       " 'hooper',\n",
       " 'burch',\n",
       " 'mullen',\n",
       " 'fry',\n",
       " 'riddle',\n",
       " 'levy',\n",
       " 'david',\n",
       " 'duke',\n",
       " 'odonnell',\n",
       " 'guy',\n",
       " 'michael',\n",
       " 'britt',\n",
       " 'frederick',\n",
       " 'daugherty',\n",
       " 'berger',\n",
       " 'dillard',\n",
       " 'alston',\n",
       " 'jarvis',\n",
       " 'frye',\n",
       " 'riggs',\n",
       " 'chaney',\n",
       " 'odom',\n",
       " 'duffy',\n",
       " 'fitzpatrick',\n",
       " 'valenzuela',\n",
       " 'merrill',\n",
       " 'mayer',\n",
       " 'alford',\n",
       " 'mcpherson',\n",
       " 'acevedo',\n",
       " 'donovan',\n",
       " 'barrera',\n",
       " 'albert',\n",
       " 'cote',\n",
       " 'reilly',\n",
       " 'compton',\n",
       " 'raymond',\n",
       " 'mooney',\n",
       " 'mcgowan',\n",
       " 'craft',\n",
       " 'cleveland',\n",
       " 'clemons',\n",
       " 'wynn',\n",
       " 'nielsen',\n",
       " 'baird',\n",
       " 'stanton',\n",
       " 'snider',\n",
       " 'rosales',\n",
       " 'bright',\n",
       " 'witt',\n",
       " 'stuart',\n",
       " 'hays',\n",
       " 'holden',\n",
       " 'rutledge',\n",
       " 'kinney',\n",
       " 'clements',\n",
       " 'castaneda',\n",
       " 'slater',\n",
       " 'hahn',\n",
       " 'emerson',\n",
       " 'conrad',\n",
       " 'burks',\n",
       " 'delaney',\n",
       " 'pate',\n",
       " 'lancaster',\n",
       " 'sweet',\n",
       " 'justice',\n",
       " 'tyson',\n",
       " 'sharpe',\n",
       " 'whitfield',\n",
       " 'talley',\n",
       " 'macias',\n",
       " 'irwin',\n",
       " 'burris',\n",
       " 'ratliff',\n",
       " 'mccray',\n",
       " 'madden',\n",
       " 'kaufman',\n",
       " 'beach',\n",
       " 'goff',\n",
       " 'cash',\n",
       " 'bolton',\n",
       " 'mcfadden',\n",
       " 'levine',\n",
       " 'good',\n",
       " 'byers',\n",
       " 'kirkland',\n",
       " 'kidd',\n",
       " 'workman',\n",
       " 'carney',\n",
       " 'dale',\n",
       " 'mcleod',\n",
       " 'holcomb',\n",
       " 'england',\n",
       " 'finch',\n",
       " 'head',\n",
       " 'burt',\n",
       " 'hendrix',\n",
       " 'sosa',\n",
       " 'haney',\n",
       " 'franks',\n",
       " 'sargent',\n",
       " 'nieves',\n",
       " 'downs',\n",
       " 'rasmussen',\n",
       " 'bird',\n",
       " 'hewitt',\n",
       " 'lindsay',\n",
       " 'le',\n",
       " 'foreman',\n",
       " 'valencia',\n",
       " 'oneil',\n",
       " 'delacruz',\n",
       " 'vinson',\n",
       " 'dejesus',\n",
       " 'hyde',\n",
       " 'forbes',\n",
       " 'gilliam',\n",
       " 'guthrie',\n",
       " 'wooten',\n",
       " 'huber',\n",
       " 'barlow',\n",
       " 'boyle',\n",
       " 'mcmahon',\n",
       " 'buckner',\n",
       " 'rocha',\n",
       " 'puckett',\n",
       " 'langley',\n",
       " 'knowles',\n",
       " 'cooke',\n",
       " 'velazquez',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "stop_words_files = ['StopWords_Generic.txt',\n",
    "                    'StopWords_Names.txt',\n",
    "                    'StopWords_DatesandNumbers.txt',\n",
    "                    'StopWords_Auditor.txt',\n",
    "                    'StopWords_GenericLong.txt',\n",
    "                    'StopWords_Currencies.txt',\n",
    "                    'StopWords_Geographic.txt']\n",
    "\n",
    "with open(\"stop_words.txt\", \"w\") as s:\n",
    "    for file in stop_words_files:\n",
    "        with open(file, 'r') as f:\n",
    "            contents = f.read()\n",
    "            s.write(contents)\n",
    "# extracting raw text of stop words\n",
    "s =  open(\"stop_words.txt\", \"r\")\n",
    "stop_words_txt = s.read()\n",
    "s.close()\n",
    "\n",
    "raw_stop_list = re.split(r'[| \\n]\\s*', stop_words_txt.lower())\n",
    "stop_list = [word for word in raw_stop_list if word.isalpha()]\n",
    "stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code https://monkeylearn.com/blog/text-cleaning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text in ( `file_to_list()` ) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns cleans text and \n",
    "# returns words containig only alphabets in file in a list\n",
    "# args: file_name => str\n",
    "# return list\n",
    "def file_to_list(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        text = f.read()\n",
    "        corp = re.sub('[^a-zA-Z]+',' ', text).strip()\n",
    "        corp = str(corp).lower()\n",
    "        tokens = nltk.word_tokenize(corp)     \n",
    "        return (tokens)\n",
    "\n",
    "# function that returns number of sentences in a file\n",
    "# args: file_name => str\n",
    "# returns: len(sentences) => int\n",
    "def no_of_sentences(file_name):\n",
    "    with open(file_name,'r') as f1:\n",
    "        para = f1.read()\n",
    "    sentences = nltk.sent_tokenize(para)\n",
    "    return len(sentences)\n",
    "\n",
    "# args: word => str, \n",
    "# returns : lnumbe of syllables => int\n",
    "def syllables(word):\n",
    "    pyp = pyphen.Pyphen(lang='en')\n",
    "    syll = pyp.inserted(word)\n",
    "    # print(syll)\n",
    "    return( len(syll.split(\"-\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding the `stop_list` words and\n",
    "### Calculaing scores and other variables for each txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate scores\n",
    "# args: \n",
    "#   file_name => str,\n",
    "#   stop => list (containing stop words), \n",
    "#   dictionary => dict (available dictionary of positive and negative words)\n",
    "# returns: scores => dict\n",
    "def scores(file_name, dictionary, stop_list):\n",
    "\n",
    "    # extract list of words from the txt file\n",
    "    file = file_to_list(file_name)\n",
    "\n",
    "    # exclude words in stop file and store in stop_excluded list\n",
    "    stop_excluded = set(file).difference(set(stop_list))\n",
    "\n",
    "    # get list of elements common between file and positive\n",
    "    pos_in_file = list( set.intersection(stop_excluded , \n",
    "                                        set(dictionary[\"positive\"])) )\n",
    "    # get list of elements common between file and negative\n",
    "    neg_in_file = list( set.intersection(stop_excluded , \n",
    "                                        set(dictionary[\"negative\"])))\n",
    "\n",
    "    # We count the total cleaned words present in the text by \n",
    "    # 1. removing the stop words (using stopwords class of nltk package).\n",
    "    # 2. removing any punctuations like ? ! , . from the word before counting.\n",
    "    file = list(stop_excluded)\n",
    "    no_of_words = len(file)\n",
    "\n",
    "    # scores\n",
    "    pos_score = len(pos_in_file)\n",
    "    neg_score = len(neg_in_file)\n",
    "    # Polarity Score = (Positive Score – Negative Score)\n",
    "    #                                       / \n",
    "    #                        ((Positive Score + Negative Score) + 0.000001)\n",
    "    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "\n",
    "    # Subjectivity Score = (Positive Score + Negative Score)\n",
    "    #                                   / \n",
    "    #                       ((Total Words after cleaning) + 0.000001)\n",
    "    subjectivity_score = (pos_score + neg_score) / (no_of_words + 0.000001)\n",
    "\n",
    "    # Average Number of Words Per Sentence (average sentence length)\n",
    "    words_per_sentence = no_of_words / no_of_sentences(file_name)\n",
    "\n",
    "    # Analysis of Readability (Gunning Fog index)\n",
    "    # Average Sentence Length = the number of words / the number of sentences\n",
    "    # Percentage of Complex words = the number of complex words / the number of words \n",
    "    # Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "    # Complex Word Count\n",
    "    complex_words = [w for w in file if syllables(w) > 2 ]\n",
    "    percent_complex_words = len(complex_words) / no_of_words\n",
    "    fog_index = 0.4 * (words_per_sentence + percent_complex_words)\n",
    "\n",
    "    # Syllable Count Per Word\n",
    "    syllables_per_word = sum([syllables(w) for w in file]) / no_of_words\n",
    "\n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = []\n",
    "    for i in range(len(file)):\n",
    "        if file[i] in ['i', 'we', 'my', 'ours']:\n",
    "            personal_pronouns.append(file[i])\n",
    "\n",
    "        # to exclude the US (country)\n",
    "        elif file[i] == 'us' and file[i-1] != 'the':\n",
    "            personal_pronouns.append(file[i])\n",
    "\n",
    "    # Average Word Length\n",
    "    # Average Word Length is calculated by the formula:\n",
    "    # Sum of the total number of characters in each word/Total number of words\n",
    "    sum_char = 0\n",
    "    for word in [\"I\", \"am\",\"good\",\"boy\"]:\n",
    "        sum_char += len(word)\n",
    "\n",
    "    avg_word_len = sum_char / no_of_words\n",
    "\n",
    "    # dict of scores\n",
    "    scores = {\n",
    "        \"POSITIVE SCORE\": pos_score,\n",
    "        \"NEGATIVE SCORE\": neg_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": words_per_sentence,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percent_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": len(complex_words),\n",
    "        \"WORD COUNT\": no_of_words,\n",
    "        \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "        \"PERSONAL PRONOUNS\": len(personal_pronouns),\n",
    "        \"AVG WORD LENGTH\": avg_word_len \n",
    "    }\n",
    "\n",
    "    return (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the output.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>0.314554</td>\n",
       "      <td>5.805822</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>67</td>\n",
       "      <td>213</td>\n",
       "      <td>2.150235</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.138614</td>\n",
       "      <td>4.697674</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.980060</td>\n",
       "      <td>4.697674</td>\n",
       "      <td>51</td>\n",
       "      <td>202</td>\n",
       "      <td>1.980198</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>7.253333</td>\n",
       "      <td>0.336397</td>\n",
       "      <td>3.035892</td>\n",
       "      <td>7.253333</td>\n",
       "      <td>183</td>\n",
       "      <td>544</td>\n",
       "      <td>2.181985</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.098814</td>\n",
       "      <td>10.120000</td>\n",
       "      <td>0.328063</td>\n",
       "      <td>4.179225</td>\n",
       "      <td>10.120000</td>\n",
       "      <td>166</td>\n",
       "      <td>506</td>\n",
       "      <td>2.162055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.131086</td>\n",
       "      <td>8.754098</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>3.639467</td>\n",
       "      <td>8.754098</td>\n",
       "      <td>184</td>\n",
       "      <td>534</td>\n",
       "      <td>2.176030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.148936</td>\n",
       "      <td>0.135057</td>\n",
       "      <td>9.405405</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>3.879404</td>\n",
       "      <td>9.405405</td>\n",
       "      <td>102</td>\n",
       "      <td>348</td>\n",
       "      <td>2.034483</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>9.102041</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>3.749337</td>\n",
       "      <td>9.102041</td>\n",
       "      <td>121</td>\n",
       "      <td>446</td>\n",
       "      <td>2.008969</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.217391</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>5.719298</td>\n",
       "      <td>0.331288</td>\n",
       "      <td>2.420235</td>\n",
       "      <td>5.719298</td>\n",
       "      <td>108</td>\n",
       "      <td>326</td>\n",
       "      <td>2.082822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>7.812367</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>104</td>\n",
       "      <td>268</td>\n",
       "      <td>2.302239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.269231</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>5.096774</td>\n",
       "      <td>0.291139</td>\n",
       "      <td>2.155165</td>\n",
       "      <td>5.096774</td>\n",
       "      <td>92</td>\n",
       "      <td>316</td>\n",
       "      <td>2.091772</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0                20               5        0.600000            0.117371   \n",
       "1                19               9        0.357143            0.138614   \n",
       "2                48              22        0.371429            0.128676   \n",
       "3                36              14        0.440000            0.098814   \n",
       "4                44              26        0.257143            0.131086   \n",
       "..              ...             ...             ...                 ...   \n",
       "145              20              27       -0.148936            0.135057   \n",
       "146              21              11        0.312500            0.071749   \n",
       "147              18              28       -0.217391            0.141104   \n",
       "148              20               7        0.481481            0.100746   \n",
       "149              19              33       -0.269231            0.164557   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              14.200000                     0.314554   5.805822   \n",
       "1               4.697674                     0.252475   1.980060   \n",
       "2               7.253333                     0.336397   3.035892   \n",
       "3              10.120000                     0.328063   4.179225   \n",
       "4               8.754098                     0.344569   3.639467   \n",
       "..                   ...                          ...        ...   \n",
       "145             9.405405                     0.293103   3.879404   \n",
       "146             9.102041                     0.271300   3.749337   \n",
       "147             5.719298                     0.331288   2.420235   \n",
       "148            19.142857                     0.388060   7.812367   \n",
       "149             5.096774                     0.291139   2.155165   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           14.200000                  67         213   \n",
       "1                            4.697674                  51         202   \n",
       "2                            7.253333                 183         544   \n",
       "3                           10.120000                 166         506   \n",
       "4                            8.754098                 184         534   \n",
       "..                                ...                 ...         ...   \n",
       "145                          9.405405                 102         348   \n",
       "146                          9.102041                 121         446   \n",
       "147                          5.719298                 108         326   \n",
       "148                         19.142857                 104         268   \n",
       "149                          5.096774                  92         316   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             2.150235                  0         0.046948  \n",
       "1             1.980198                  0         0.049505  \n",
       "2             2.181985                  0         0.018382  \n",
       "3             2.162055                  0         0.019763  \n",
       "4             2.176030                  0         0.018727  \n",
       "..                 ...                ...              ...  \n",
       "145           2.034483                  0         0.028736  \n",
       "146           2.008969                  0         0.022422  \n",
       "147           2.082822                  0         0.030675  \n",
       "148           2.302239                  0         0.037313  \n",
       "149           2.091772                  0         0.031646  \n",
       "\n",
       "[150 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(1,151):\n",
    "    txt_file = str(i) + '.txt'\n",
    "    data.append( scores(txt_file, dictionary, stop_list) )\n",
    "\n",
    "score_df = pd.DataFrame( data )\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>0.314554</td>\n",
       "      <td>5.805822</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>67</td>\n",
       "      <td>213</td>\n",
       "      <td>2.150235</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-telehealt...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.138614</td>\n",
       "      <td>4.697674</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>1.980060</td>\n",
       "      <td>4.697674</td>\n",
       "      <td>51</td>\n",
       "      <td>202</td>\n",
       "      <td>1.980198</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telemedici...</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>7.253333</td>\n",
       "      <td>0.336397</td>\n",
       "      <td>3.035892</td>\n",
       "      <td>7.253333</td>\n",
       "      <td>183</td>\n",
       "      <td>544</td>\n",
       "      <td>2.181985</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.098814</td>\n",
       "      <td>10.120000</td>\n",
       "      <td>0.328063</td>\n",
       "      <td>4.179225</td>\n",
       "      <td>10.120000</td>\n",
       "      <td>166</td>\n",
       "      <td>506</td>\n",
       "      <td>2.162055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-people-di...</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.131086</td>\n",
       "      <td>8.754098</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>3.639467</td>\n",
       "      <td>8.754098</td>\n",
       "      <td>184</td>\n",
       "      <td>534</td>\n",
       "      <td>2.176030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/is-telehealth...              20   \n",
       "1       2  https://insights.blackcoffer.com/how-telehealt...              19   \n",
       "2       3  https://insights.blackcoffer.com/is-telemedici...              48   \n",
       "3       4  https://insights.blackcoffer.com/is-telehealth...              36   \n",
       "4       5  https://insights.blackcoffer.com/how-people-di...              44   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               5        0.600000            0.117371            14.200000   \n",
       "1               9        0.357143            0.138614             4.697674   \n",
       "2              22        0.371429            0.128676             7.253333   \n",
       "3              14        0.440000            0.098814            10.120000   \n",
       "4              26        0.257143            0.131086             8.754098   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                     0.314554   5.805822                         14.200000   \n",
       "1                     0.252475   1.980060                          4.697674   \n",
       "2                     0.336397   3.035892                          7.253333   \n",
       "3                     0.328063   4.179225                         10.120000   \n",
       "4                     0.344569   3.639467                          8.754098   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                  67         213           2.150235                  0   \n",
       "1                  51         202           1.980198                  0   \n",
       "2                 183         544           2.181985                  0   \n",
       "3                 166         506           2.162055                  0   \n",
       "4                 184         534           2.176030                  0   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         0.046948  \n",
       "1         0.049505  \n",
       "2         0.018382  \n",
       "3         0.019763  \n",
       "4         0.018727  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.concat([df, score_df], axis=1)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.csv', 'w') as output_file:\n",
    "    output_file.write(output.to_csv(index=False, line_terminator='\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d623e3832b78ce69fa2c095939a54ed4f3e8fe6909ed4ae68f2da80f027625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
